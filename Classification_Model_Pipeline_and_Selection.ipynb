{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMxVT2DyDhgZP4E/7qbkap",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/briangeorg/Machine_Learning_Portfolio/blob/main/Classification_Model_Pipeline_and_Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What's Our Project?**\n",
        "\n",
        "We're working cross-functionally with HR to evaluate employee data; the project is to predict which employees are most likely to leave the company, enabling the organization to implement a retention program."
      ],
      "metadata": {
        "id": "dmIe4r8hIOpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting Started**\n",
        "\n",
        "Below we'll ingest our raw data from Kaggle and wrangle our data into a format that's usable across our intended model types."
      ],
      "metadata": {
        "id": "VIx2w8gUH9Jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies as needed:\n",
        "# pip install kagglehub[pandas-datasets]\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Set the path to the file you'd like to load\n",
        "file_path = \"WA_Fn-UseC_-HR-Employee-Attrition.csv\"\n",
        "\n",
        "# Load the latest version\n",
        "df = kagglehub.dataset_load(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"pavansubhasht/ibm-hr-analytics-attrition-dataset\",\n",
        "  file_path\n",
        ")\n",
        "# View csv data below for a basic understanding:\n",
        "print(\"First 5 records:\", df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDwOS4NUIZbD",
        "outputId": "357236dd-eee6-47df-bf33-1b1f74cc0303"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'ibm-hr-analytics-attrition-dataset' dataset.\n",
            "First 5 records:    Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
            "0   41       Yes      Travel_Rarely       1102                   Sales   \n",
            "1   49        No  Travel_Frequently        279  Research & Development   \n",
            "2   37       Yes      Travel_Rarely       1373  Research & Development   \n",
            "3   33        No  Travel_Frequently       1392  Research & Development   \n",
            "4   27        No      Travel_Rarely        591  Research & Development   \n",
            "\n",
            "   DistanceFromHome  Education EducationField  EmployeeCount  EmployeeNumber  \\\n",
            "0                 1          2  Life Sciences              1               1   \n",
            "1                 8          1  Life Sciences              1               2   \n",
            "2                 2          2          Other              1               4   \n",
            "3                 3          4  Life Sciences              1               5   \n",
            "4                 2          1        Medical              1               7   \n",
            "\n",
            "   ...  RelationshipSatisfaction StandardHours  StockOptionLevel  \\\n",
            "0  ...                         1            80                 0   \n",
            "1  ...                         4            80                 1   \n",
            "2  ...                         2            80                 0   \n",
            "3  ...                         3            80                 0   \n",
            "4  ...                         4            80                 1   \n",
            "\n",
            "   TotalWorkingYears  TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n",
            "0                  8                      0               1               6   \n",
            "1                 10                      3               3              10   \n",
            "2                  7                      3               3               0   \n",
            "3                  8                      3               3               8   \n",
            "4                  6                      3               3               2   \n",
            "\n",
            "  YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \n",
            "0                  4                        0                     5  \n",
            "1                  7                        1                     7  \n",
            "2                  0                        0                     0  \n",
            "3                  7                        3                     0  \n",
            "4                  2                        2                     2  \n",
            "\n",
            "[5 rows x 35 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Prep**\n",
        "\n",
        "First, we'll do three things:\n",
        "\n",
        "1. Remove variables we identified as non-meaningful in our Exploratory Data Analysis workbook (these were Employee Count and Standard Hours)\n",
        "\n",
        "2. Min-max scale our numeric factors\n",
        "\n",
        "2. Recode (one-hot encoding) our categorical variables"
      ],
      "metadata": {
        "id": "z0IS0YBEItZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove non-meaningful factors (due to zero variance):\n",
        "\n",
        "columns_to_remove = ['EmployeeCount', 'StandardHours', 'Over18']\n",
        "df_clean = df.drop(columns=columns_to_remove)\n",
        "\n",
        "# Use sklearn's MinMaxScaler function to scale:\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "numeric_cols = df_clean.select_dtypes(include=np.number).columns\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "df_clean[numeric_cols] = scaler.fit_transform(df_clean[numeric_cols])\n",
        "\n",
        "# Use Pandas' get dummies function to one-hot encode our categorical variables:\n",
        "\n",
        "df_encoded = pd.get_dummies(df_clean, dtype=int)\n",
        "\n",
        "print(df_encoded.head())\n",
        "\n",
        "# Finally: we have two factors where we were one-hot encoding a binary string.\n",
        "# These were out target (Attrition) and another predictor (Overtime); for these,\n",
        "# we will want to drop-out the 'no' variants of each, as they're redundant and\n",
        "# should not be included.\n",
        "\n",
        "columns_to_remove = ['Attrition_No', 'OverTime_No']\n",
        "df_final = df_encoded.drop(columns=columns_to_remove)\n"
      ],
      "metadata": {
        "id": "aqi36W2o4RwE",
        "outputId": "ce6d7567-f37c-4fc7-89e9-096640e038ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Age  DailyRate  DistanceFromHome  Education  EmployeeNumber  \\\n",
            "0  0.547619   0.715820          0.000000       0.25        0.000000   \n",
            "1  0.738095   0.126700          0.250000       0.00        0.000484   \n",
            "2  0.452381   0.909807          0.035714       0.25        0.001451   \n",
            "3  0.357143   0.923407          0.071429       0.75        0.001935   \n",
            "4  0.214286   0.350036          0.035714       0.00        0.002903   \n",
            "\n",
            "   EnvironmentSatisfaction  HourlyRate  JobInvolvement  JobLevel  \\\n",
            "0                 0.333333    0.914286        0.666667      0.25   \n",
            "1                 0.666667    0.442857        0.333333      0.25   \n",
            "2                 1.000000    0.885714        0.333333      0.00   \n",
            "3                 1.000000    0.371429        0.666667      0.00   \n",
            "4                 0.000000    0.142857        0.666667      0.00   \n",
            "\n",
            "   JobSatisfaction  ...  JobRole_Research Director  \\\n",
            "0         1.000000  ...                          0   \n",
            "1         0.333333  ...                          0   \n",
            "2         0.666667  ...                          0   \n",
            "3         0.666667  ...                          0   \n",
            "4         0.333333  ...                          0   \n",
            "\n",
            "   JobRole_Research Scientist  JobRole_Sales Executive  \\\n",
            "0                           0                        1   \n",
            "1                           1                        0   \n",
            "2                           0                        0   \n",
            "3                           1                        0   \n",
            "4                           0                        0   \n",
            "\n",
            "   JobRole_Sales Representative  MaritalStatus_Divorced  \\\n",
            "0                             0                       0   \n",
            "1                             0                       0   \n",
            "2                             0                       0   \n",
            "3                             0                       0   \n",
            "4                             0                       0   \n",
            "\n",
            "   MaritalStatus_Married  MaritalStatus_Single  Over18_Y  OverTime_No  \\\n",
            "0                      0                     1         1            0   \n",
            "1                      1                     0         1            1   \n",
            "2                      0                     1         1            0   \n",
            "3                      1                     0         1            0   \n",
            "4                      1                     0         1            1   \n",
            "\n",
            "   OverTime_Yes  \n",
            "0             1  \n",
            "1             0  \n",
            "2             1  \n",
            "3             1  \n",
            "4             0  \n",
            "\n",
            "[5 rows x 55 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting Our Data into Trainin and Test Sets**:\n",
        "\n",
        "It's important to note here that, normally, we would also want a validation\n",
        "set to allow us to run hyperparameter tuning before we train our models!\n",
        "\n",
        "However, in this use case, we find ourselves with a (very) small data set: by relying on k-fold cross validation, we can do our utmost to ensure our models are generalizable to unseen data. But here we will not split-out a validation set, as the tradeoff might be a substantial decrease in our model's predictive power."
      ],
      "metadata": {
        "id": "4CSrE7UT-nZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we'll split our data into training and test sets so\n",
        "# that we're ready to model:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pSrsPuFG9wq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training**:\n",
        "\n",
        "Here we'll implement 3 contenders (well, two serious contenders and a benchmark model):\n",
        "\n",
        "1. A logistic regression model - this is to serve as a benchmark or baseline of comparison to understand our other proposed models; we don't just want to know how well they perform, but how much better are they than a simple, linear decision boundary\n",
        "\n",
        "2. Light GBM [Gradient Boosted Machine]: this implementation of"
      ],
      "metadata": {
        "id": "9rlSZ2SVQtTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "'''\n",
        "***Important Note: altering the kf values below will affect whether\n",
        "    your CV results are reproducible (decides how your CV partitions\n",
        "    are set)***\n",
        "'''\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Baseline/Benchmark Logistic Regression Model:\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logit_scores = []\n",
        "\n",
        "for train_idx, test_idx in kf.split(X):\n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    logit_scores.append(accuracy_score(y_test, preds))\n",
        "\n",
        "# Light GBM Model:\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "lightgbm_scores = []\n",
        "\n",
        "for train_idx, test_idx in kf.split(X):\n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "    train_data = lgb.Dataset(X_train, label=y_train)\n",
        "    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
        "\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'binary_error',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': 31,\n",
        "        'learning_rate': 0.05,\n",
        "        'verbose': -1\n",
        "    }\n",
        "\n",
        "    model = lgb.train(params, train_data, valid_sets=[test_data], early_stopping_rounds=10, verbose_eval=False)\n",
        "    preds = model.predict(X_test)\n",
        "    preds_binary = (preds > 0.5).astype(int)\n",
        "    lightgbm_scores.append(accuracy_score(y_test, preds_binary))\n",
        "\n",
        "# Deep Neural Network (DNN) using TensorFlow\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "dnn_scores = []\n",
        "\n",
        "for train_idx, test_idx in kf.split(X):\n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "    _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    dnn_scores.append(accuracy)\n",
        "\n",
        "# Finally, we'll compare the models' performance, focusing on accuracy:\n",
        "\n",
        "print(\"LightGBM Average Accuracy:\", np.mean(lightgbm_scores))\n",
        "print(\"Logistic Regression Average Accuracy:\", np.mean(logit_scores))\n",
        "print(\"DNN Average Accuracy:\", np.mean(dnn_scores))"
      ],
      "metadata": {
        "id": "RRRtKeJLQrgu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}