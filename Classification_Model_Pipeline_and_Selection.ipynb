{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMF8wjKZ8EpRidCoVQ9GReC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/briangeorg/Machine_Learning_Portfolio/blob/main/Classification_Model_Pipeline_and_Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What's Our Project?**\n",
        "\n",
        "We're working cross-functionally with HR to evaluate employee data; the project is to predict which employees are most likely to leave the company, enabling the organization to implement a retention program."
      ],
      "metadata": {
        "id": "dmIe4r8hIOpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting Started**\n",
        "\n",
        "Below we'll ingest our raw data from Kaggle and wrangle our data into a format that's usable across our intended model types."
      ],
      "metadata": {
        "id": "VIx2w8gUH9Jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies as needed:\n",
        "# pip install kagglehub[pandas-datasets]\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Set the path to the file you'd like to load\n",
        "file_path = \"WA_Fn-UseC_-HR-Employee-Attrition.csv\"\n",
        "\n",
        "# Load the latest version\n",
        "df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"pavansubhasht/ibm-hr-analytics-attrition-dataset\",\n",
        "  file_path\n",
        ")\n",
        "# View csv data below for a basic understanding:\n",
        "print(\"First 5 records:\", df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDwOS4NUIZbD",
        "outputId": "17bf0fee-985e-4813-ddeb-a3caea7ba8e8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3932391491.py:10: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'ibm-hr-analytics-attrition-dataset' dataset.\n",
            "First 5 records:    Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
            "0   41       Yes      Travel_Rarely       1102                   Sales   \n",
            "1   49        No  Travel_Frequently        279  Research & Development   \n",
            "2   37       Yes      Travel_Rarely       1373  Research & Development   \n",
            "3   33        No  Travel_Frequently       1392  Research & Development   \n",
            "4   27        No      Travel_Rarely        591  Research & Development   \n",
            "\n",
            "   DistanceFromHome  Education EducationField  EmployeeCount  EmployeeNumber  \\\n",
            "0                 1          2  Life Sciences              1               1   \n",
            "1                 8          1  Life Sciences              1               2   \n",
            "2                 2          2          Other              1               4   \n",
            "3                 3          4  Life Sciences              1               5   \n",
            "4                 2          1        Medical              1               7   \n",
            "\n",
            "   ...  RelationshipSatisfaction StandardHours  StockOptionLevel  \\\n",
            "0  ...                         1            80                 0   \n",
            "1  ...                         4            80                 1   \n",
            "2  ...                         2            80                 0   \n",
            "3  ...                         3            80                 0   \n",
            "4  ...                         4            80                 1   \n",
            "\n",
            "   TotalWorkingYears  TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n",
            "0                  8                      0               1               6   \n",
            "1                 10                      3               3              10   \n",
            "2                  7                      3               3               0   \n",
            "3                  8                      3               3               8   \n",
            "4                  6                      3               3               2   \n",
            "\n",
            "  YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \n",
            "0                  4                        0                     5  \n",
            "1                  7                        1                     7  \n",
            "2                  0                        0                     0  \n",
            "3                  7                        3                     0  \n",
            "4                  2                        2                     2  \n",
            "\n",
            "[5 rows x 35 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Prep**\n",
        "\n",
        "First, we'll do two things:\n",
        "\n",
        "1. Remove variables we identified as non-meaningful in our Exploratory Data Analysis workbook\n",
        "\n",
        "2. Recode (one-hot encoding) our categorical variables, and implement min-max scaling across our dataset so that, regardless of which model type we implement, the data are uniformly usable (scaling your data is unnecessary in some model types, but in others, it may create serious problems!)"
      ],
      "metadata": {
        "id": "z0IS0YBEItZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training**:\n",
        "\n",
        "Here we'll implement 3 contenders (well, two serious contenders and a benchmark model):\n",
        "\n",
        "1. A logistic regression model - this is to serve as a benchmark or baseline of comparison to understand our other proposed models; we don't just want to know how well they perform, but how much better are they than a simple, linear decision boundary\n",
        "\n",
        "2. Light GBM [Gradient Boosted Machine]: this implementation of"
      ],
      "metadata": {
        "id": "9rlSZ2SVQtTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "'''\n",
        "***Important Note: altering the kf values below will affect whether\n",
        "    your CV results are reproducible (decides how your CV partitions\n",
        "    are set)***\n",
        "'''\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Baseline/Benchmark Logistic Regression Model:\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logit_scores = []\n",
        "\n",
        "for train_idx, test_idx in kf.split(X):\n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    logit_scores.append(accuracy_score(y_test, preds))\n",
        "\n",
        "# Light GBM Model:\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "lightgbm_scores = []\n",
        "\n",
        "for train_idx, test_idx in kf.split(X):\n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "    train_data = lgb.Dataset(X_train, label=y_train)\n",
        "    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
        "\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'binary_error',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': 31,\n",
        "        'learning_rate': 0.05,\n",
        "        'verbose': -1\n",
        "    }\n",
        "\n",
        "    model = lgb.train(params, train_data, valid_sets=[test_data], early_stopping_rounds=10, verbose_eval=False)\n",
        "    preds = model.predict(X_test)\n",
        "    preds_binary = (preds > 0.5).astype(int)\n",
        "    lightgbm_scores.append(accuracy_score(y_test, preds_binary))\n",
        "\n",
        "# Deep Neural Network (DNN) using TensorFlow\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "dnn_scores = []\n",
        "\n",
        "for train_idx, test_idx in kf.split(X):\n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "    _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    dnn_scores.append(accuracy)\n",
        "\n",
        "# Finally, we'll compare the models' performance, focusing on accuracy:\n",
        "\n",
        "print(\"LightGBM Average Accuracy:\", np.mean(lightgbm_scores))\n",
        "print(\"Logistic Regression Average Accuracy:\", np.mean(logit_scores))\n",
        "print(\"DNN Average Accuracy:\", np.mean(dnn_scores))"
      ],
      "metadata": {
        "id": "RRRtKeJLQrgu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}